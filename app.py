"""
This module is a Streamlit application that utilizes Google GenAI for benchmarking large language models.
It imports necessary libraries for data manipulation (pandas), visualization (plotly.express), and time management.

Modules:
- streamlit: For creating the web application interface.
- google.genai: For accessing Googleâ€™s Generative AI functionalities.
- time: For handling time-related tasks.
- pandas: For data manipulation and analysis.
- plotly.express: For creating interactive visualizations.

Filepath:
- /home/chisom/codex/llm-benchmarking/app.py
"""

import streamlit as st
from google import genai
from groq import Groq
import time
import pandas as pd
import plotly.express as px

st.set_page_config(page_title="LLM Benchmarking", layout="wide")

st.title("LLM Benchmarking")
st.subheader("Compare as many LLMs as you want side by side")
st.divider()

client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
groq_client = Groq(api_key=st.secrets["GROQ_API_KEY"])

def call_gemini(prompt):
    start = time.time()
    response = client.models.generate_content(model="gemini-2.5-flash",
                                              contents=prompt)
    end = time.time()
    if response.usage_metadata:
        token_count = response.usage_metadata.total_token_count
    else:
        token_count = len(response.text) // 4  # Rough estimate if metadata is unavailable
    latency = end - start
    return response.text, latency, token_count

def call_llama(prompt):
    start = time.time()
    response_groq = groq_client.chat.completions.create(model="llama-3.1-8b-instant",
                                                   messages=[{"role": "user", "content": prompt}], temperature=0.5)
    end = time.time()
    content = response_groq.choices[0].message
    token_count = response_groq.usage.total_tokens
    return content, end - start, token_count

with st.sidebar:
    st.title("Choose models")
    use_gemini = st.checkbox("Gemini 2.5 Flash", value=True)
    use_groq = st.checkbox("Llama-3.1", value=True)
    
prompt = st.chat_input("Enter your prompt")

if prompt:
    comparisions =[]
    if use_gemini:
        comparisions.append("Gemini 2.5 Flash")
    if use_groq:
        comparisions.append("Llama-3.1")
        
    cols = st.columns(len(comparisions))
    results = []
    
    for i, comparision_name in enumerate(comparisions):
        with cols[i]:
            st.subheader(comparision_name)
            if comparision_name == "Gemini 2.5 Flash":
                content, latency, tokens=call_gemini(prompt)
            else:
                content, latency, tokens=call_llama(prompt)
        st.caption(f"Response Time(Latency): {latency:.2f} seconds | Tokens Used: {tokens}")
        st.write(content)
        
        if latency > 0:
            results.append({
                "Model": comparision_name,
                "Latency (s)": latency,
                "Tokens Used": tokens,
                "Throughput (tokens/s)": tokens / latency,
                "Cost (USD)": (tokens / 1000) * 0.03
            })
    
    if results:
        df = pd.DataFrame(results)
        st.subheader("Benchmark Results")
        st.dataframe(df)
        
        fig = px.bar(df, x="Model", y="Throughput (tokens/s)", title="Model Throughput Comparison")
        st.plotly_chart(fig, use_container_width=True)
        
        # Added Latency Comparison Chart
        fig = px.bar(df, x="Model", y="Latency (s)", title="Model Latency Comparison")
        st.plotly_chart(fig, use_container_width=True)
        
        # Added Cost Comparison Chart
        fig = px.bar(df, x="Model", y="Cost (USD)", title="Model Cost Comparison")
        st.plotly_chart(fig, use_container_width=True)
        