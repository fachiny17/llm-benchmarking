# Benchmarking multiple LLMs in Streamlit
Learn how to compare large language models side-by-side using Streamlit! In this session, you’ll build an interactive dashboard that sends prompts to multiple LLMs, measures their performance, and visualizes the results. A practical way to evaluate speed, quality, and cost so you can choose the right model for your projects.

## Features
- ✅ Understand the different models
- ✅ Learn about benchmarking
- ✅ Commonly used metrics
- ✅ Construct a nice interface with Streamlit
- ✅ Deploy!

## Run Streamlit

1. Isntall Steamlit: `pip install streamlit`

`streamlit run app.py`